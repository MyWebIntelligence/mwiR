% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/recode.R
\name{LLM_Recode}
\alias{LLM_Recode}
\title{Recode Data Using Large Language Models}
\usage{
LLM_Recode(
  data,
  prompt,
  provider = NULL,
  model = NULL,
  temperature = 0.7,
  max_tokens = 1000,
  sysprompt =
    "You are a helpful assistant that recodes dataframe values. Return only the transformed value.",
  api_key = NULL,
  api_base = NULL,
  timeout = 60,
  max_retries = 3,
  retry_delay = 1,
  backoff_multiplier = 2,
  rate_limit_delay = 0,
  validate = TRUE,
  return_metadata = FALSE,
  on_error = c("continue", "stop"),
  parallel = FALSE,
  workers = NULL,
  progress = TRUE,
  extra_headers = NULL
)
}
\arguments{
\item{data}{A character vector or data frame to process. For vectors, use
`{value}` in your prompt. For data frames, use column names.}

\item{prompt}{A glue-style template string. Example: `"Translate to French: {value}"`.}

\item{provider}{LLM provider: "openai", "openrouter", "anthropic", or "ollama".
If NULL, auto-detects based on available API keys.}

\item{model}{Model identifier. NULL uses provider default (e.g., "gpt-4o" for OpenAI).}

\item{temperature}{Sampling temperature (0-2). Lower = more deterministic. Default: 0.7.}

\item{max_tokens}{Maximum tokens per response. Default: 1000.}

\item{sysprompt}{System prompt guiding model behavior.}

\item{api_key}{API key. If NULL, reads from environment or asks interactively.}

\item{api_base}{Custom API endpoint URL (for local models or proxies).}

\item{timeout}{Request timeout in seconds. Default: 60.}

\item{max_retries}{Maximum retry attempts per request. Default: 3.}

\item{retry_delay}{Initial delay between retries (seconds). Default: 1.}

\item{backoff_multiplier}{Multiplier for exponential backoff. Default: 2.}

\item{rate_limit_delay}{Delay between requests (seconds). Default: 0.}

\item{validate}{Apply heuristics to detect invalid responses. Default: TRUE.}

\item{return_metadata}{If TRUE, returns data frame with metadata. Default: FALSE.}

\item{on_error}{"continue" (default) returns NA and proceeds, "stop" halts on error.}

\item{parallel}{Enable parallel processing. Default: FALSE.}

\item{workers}{Number of parallel workers. NULL = auto.}

\item{progress}{Show progress bar. Default: TRUE.}

\item{extra_headers}{Named character vector of additional HTTP headers.}
}
\value{
By default, a character vector of same length as input.
  If `return_metadata = TRUE`, a data frame with columns:
  row_index, value, status, attempts, tokens_used, error_message.
}
\description{
Transform data values using LLMs (OpenAI, OpenRouter, Anthropic, Ollama).
Designed for social science researchers: simple to use, interactive configuration,
resilient error handling, and French/English messages.

Uses glue-style templates for prompts: `"Translate {value} to French"`.
When passing a simple vector, use `{value}` as the placeholder.
When passing a data frame, use column names as placeholders.
}
\examples{
\dontrun{
# Simple translation (auto-detects provider from API keys)
result <- LLM_Recode(
  data = c("Hello world", "Good morning"),
  prompt = "Translate to French: {value}"
)

# With data frame
df <- data.frame(
  title = c("Article 1", "Article 2"),
  abstract = c("This paper explores...", "We analyze...")
)
summaries <- LLM_Recode(
  data = df,
  prompt = "Summarize in 10 words: {title} - {abstract}",
  provider = "openai"
)

# Get detailed metadata
results <- LLM_Recode(
  data = my_texts,
  prompt = "Classify sentiment (positive/negative/neutral): {value}",
  return_metadata = TRUE
)
# Check failures
failed <- results[results$status != "ok", ]

# Configure once for the session
LLM_Config(provider = "openai", lang = "fr")
# Then use simply
LLM_Recode(my_data, "Traduire: {value}")
}

}
\seealso{
\code{\link{LLM_Config}} for session configuration.
}
