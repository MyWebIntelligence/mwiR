---
output: md_document
---

# mwiR 0.8.0 (Beta) The R Package of My Web Intelligence Project

<!-- badges: start -->

<!-- badges: end -->

## Purpose of My Web Intelligence

**Context and Objectives**

My Web Intelligence (MWI) is a project designed to meet the growing need for tools and methodologies in the field of digital methods in social sciences and information and communication sciences (ICS). The main objective is to map the digital ecosystem to identify key actors, assess their influence, and analyze their discourses and interactions. This project addresses the increasing centrality of digital information and interactions in various fields, including health, politics, culture, and beyond.

### About the Author

**Amar LAKEL**

Amar Lakel is a researcher in information and communication sciences, specializing in digital methods applied to social studies. He is currently a member of the MICA laboratory (Mediation, Information, Communication, Arts) at the University of Bordeaux Montaigne. His work focuses on the analysis of online discourse, mapping digital ecosystems, and the impact of digital technologies on social and cultural practices.

#### Online Profiles

-   **MICA Labo**: [MICA Labo Profile](https://mica.u-bordeaux-montaigne.fr/amar-lakel/)
-   **Google Scholar**: [Google Scholar Profile](https://scholar.google.com/citations?user=hqquhfwAAAAJ)
-   **ORCID**: [ORCID Profile](https://orcid.org/0000-0002-1234-5678)
-   **ResearchGate**: [ResearchGate Profile](https://www.researchgate.net/profile/Amar_Lakel)
-   **Academia**: [Academia Profile](https://univ-bordeaux.academia.edu/AmarLakel)
-   **Twitter**: [Twitter MyWebIntel Profile](https://twitter.com/mywebintel)
-   **LinkedIn**: [LinkedIn Profile](https://www.linkedin.com/in/amar-lakel-123456789/)

## Methodology

**Research Protocol**

The research protocol of MWI relies on a combination of quantitative and qualitative methods:

1.  **Data Extraction and Archiving**: Using crawl technologies to collect data from the web.
2.  **Data Qualification and Annotation**: Applying algorithms to analyze, classify, and annotate the data.
3.  **Data Visualization**: Developing dashboards and relational maps to interpret the results.

**Methodological Challenges**

The MWI project utilizes techniques from the sociology of controversies, social network analysis, and text mining methods to:

-   Analyze the strategic positions of speakers in a heterogeneous and complex digital corpus.
-   Identify and understand the dynamics of online discourses.
-   Map the relationships between different actors and their respective influences.

## Case Studies

**Diverse Cases**

1.  **Health Information**

-   **Asthma and Diabetes in Children**: Studies of online discourses related to these diseases to identify influential actors, understand their positions, and evaluate their impact on patients' perceptions and behaviors. [Source](https://journals.openedition.org/rfsic/8376)

2.  **Online Political Controversy**

-   **Juan Branco Project**: Analysis of discourses and influence surrounding the public figure Juan Branco, exploring the dynamics of positioning and controversy. [Source](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4584133)

3.  **Research Sociology**

-   **Digital Humanities**: Studies on the impact of digital technologies on humanities and social sciences, including how researchers use the web to disseminate and discuss their work. [Source](https://hal.science/hal-02485370)

**Results and Impact**

The results of these studies show that online discourses play a crucial role in shaping opinions and behaviors in various fields. They also highlight the importance for researchers and professionals to actively engage in these discussions to promote reliable and scientifically validated information.

## Repositories and Documentation

**NAKALA Repositories** The data and results of the MWI project are deposited on the NAKALA platform, providing open access for other researchers and practitioners. Here are some important repositories:

1.  [The collection](https://nakala.fr/collection/10.34847/nkl.b4aarv3j): Contains a detailed description of the project, methodology, and results.
2.  [Positions and Influences on the Web: The Case of Health Information](https://nakala.fr/prise-de-position): Detailed analysis of discourses on childhood asthma.
3.  [French Digital Humanities communities](https://nakala.fr/10.34847/nkl.f43by03n): A study case on French digital humanities development on the web.

# Development of the R Package

The R package developed within the framework of My Web Intelligence is designed to: - Facilitate the replication of analyses conducted in the project. - Enable the extension of developed methods and tools for other research. - Provide researchers and professionals with a powerful tool to understand and manage the dynamics of online information.

**Main Features**

-   **Project Management**: Tools to initiate and manage web exploration projects.
-   **Data Extraction**: Functions to crawl the web and extract data corpora.
-   **Analysis and Annotation**: Algorithms to analyze and annotate extracted data.
-   **Visualization**: Dashboards and maps to visualize relationships between actors and discourses.

## Conclusion

My Web Intelligence is an integrative project aimed at transforming how we understand and analyze digital information across various fields in social sciences and ICS. By combining innovative methodologies and advanced technological tools, MWI offers new perspectives on digital dynamics and proposes solutions to better understand online interactions and discourses. The R package developed from this project is an essential tool for researchers and practitioners, enabling them to fully exploit web data for in-depth and relevant analyses.

# Using mwiR : a study case

## Installation

You can install the development version of mwiR from [GitHub](https://github.com/) with:

```{r}

# install.packages("devtools")
devtools::install_github("MyWebIntelligence/mwiR")

```

## Project ('land') Setup

This is a basic example which shows you how to solve a common problem:

```{r example}
library(mwiR)
## basic example code
```

## Step 1: Creating the Research Project

In this step-by-step guide, we will walk through the initial setup and execution of a research project using the My Web Intelligence (MWI) method. This method allows researchers to analyze the impact of various factors, such as AI on work, by collecting and organizing web data. Here is a breakdown of the R script provided:

### 1. Load the Required Packages

```{r}
initmwi()
```

The `initmwi()` function initializes the My Web Intelligence environment by loading all necessary packages and setting up the environment for further operations. This function ensures that all dependencies and configurations are correctly initialized.

### 2. Set Up the Database

```{r}
db_setup()
```

The `db_setup()` function sets up the database needed for storing and managing the data collected during the research project. It initializes the necessary database schema and ensures that the database is ready for data insertion and retrieval.

-   `db_name`: A string specifying the name of the SQLite database file. Default is `"mwi.db"`.

### 3. Create a Research Project (Land)

```{r}
create_land(name = "AIWork", desc = "Impact of AI on work", lang="en")
```

The `create_land()` function creates a new research project, referred to as a "land" in MWI terminology. This land will serve as the container for all data and analyses related to the project.

-   `name`: A string specifying the name of the land.
-   `desc`: A string providing a description of the land.
-   `lang`: A string specifying the language of the land. Default is `"en"`.
-   `db_name`: A string specifying the name of the SQLite database file. Default is `"mwi.db"`.

### 4. Add Search Terms

```{r}
addterm("AIWork", "AI, artificial intelligence, work, employment, job, profession, labor market")
```

The `addterm()` function adds search terms to the project. These terms will be used to crawl and collect relevant web data.

-   `land_name`: A string specifying the name of the land.
-   `terms`: A comma-separated string of terms to add.

### 5. Verify the Project Creation

```{r}
listlands("AIWork")
```

The `listlands()` function lists all lands or projects that have been created. By specifying the project name "AIWork", it verifies that the project has been successfully created.

-   `land_name`: A string specifying the name of the land to list. If `NULL`, all lands are listed. Default is `NULL`.
-   `db_name`: A string specifying the name of the SQLite database file. Default is `"mwi.db"`.

### 6. Add URLs Manually or Using a File

```{r}
addurl("AIWork", urls = "https://www.fr.adp.com/rhinfo/articles/2022/11/la-disparition-de-certains-metiers-est-elle-a-craindre.aspx")
```

The `addurl()` function adds URLs to the project. These URLs point to web pages that contain relevant information for the research.

-   `land_name`: A string specifying the name of the land.
-   `urls`: A comma-separated string of URLs to add. Default is `NULL`.
-   `path`: A string specifying the path to a file containing URLs. Default is `NULL`.
-   `db_name`: A string specifying the name of the SQLite database file. Default is `"mwi.db"`.

Alternatively, URLs can be added using a text file:

```{r}
# If using a text file

addurl("AIWork", path = "_ai_or_artificial_intelligence___work_or_employment_or_job_or_profession_or_labor_market01.txt")
```

-   `path`: The path to a text file containing the URLs to be added.

### 7. List the Projects or a Specific Project

```{r}
listlands("AIWork")
```

This function is used again to list the projects or a specific project, ensuring that the URLs have been added correctly to "AIWork".

### 8. Optionally Delete a Project

```{r}
deleteland(land_name = "AIWork")

```

The `deleteland()` function deletes a specified project. This can be useful for cleaning up after the research is completed or if a project needs to be restarted.

-   `land_name`: A string specifying the name of the land to delete.
-   `maxrel`: An integer specifying the maximum relevance for expressions to delete. Default is `NULL`.
-   `db_name`: A string specifying the name of the SQLite database file. Default is `"mwi.db"`.

This script demonstrates the basic setup and execution of a research project using My Web Intelligence, including project creation, term addition, URL management, and project verification.

## Step 2: Crawling

In this section, we will walk through the process of crawling URLs and extracting content for analysis using the My Web Intelligence (MWI) method. The following R code snippets demonstrate how to perform these tasks.

### Crawl URLs for a Specific Land

```{r}
crawlurls("IATravail", limit = 10)
```

The `crawlurls()` function crawls URLs for a specific land, updates the database, and calculates relevance scores.

-   `land_name`: A character string representing the name of the land.
-   `urlmax`: An integer specifying the maximum number of URLs to be processed (default is 50).
-   `limit`: An optional integer specifying the limit on the number of URLs to crawl.
-   `http_status`: An optional character string specifying the HTTP status to filter URLs.
-   `db_name`: A string specifying the name of the SQLite database file. Default is `"mwi.db"`.

**Example:**

This example demonstrates crawling up to 10 URLs for the land named "IATravail".

```{r}
crawlurls("IATravail", limit = 10)
```

### Crawl Domains

```{r}
crawlDomain(1000)
```

The `crawlDomain()` function crawls domains and updates the Domain table with the fetched data.

-   `nburl`: An integer specifying the number of URLs to be crawled (default is 100).
-   `db_name`: A string specifying the name of the SQLite database file. Default is `"mwi.db"`.

**Example:**

This example demonstrates crawling 1000 URLs and updating the Domain table.

```{r}
crawlDomain(1000)
```

## Step 3: Export Files and Corpora

In this section, we will walk through the process of exporting data and corpora from a research project using the My Web Intelligence (MWI) method. The following R code snippets demonstrate how to perform these tasks.

### Export Land Data

```{r}
#type = ['pagecsv', 'pagegexf', 'fullpagecsv', 'nodecsv', 'nodegexf', 'mediacsv', 'corpus']

# Exemple d'utilisation "le projet", "type d'export", "relevance", "file"

export_land("giletsjaunes", "pagegexf", 3)
```

The `export_land()` function manages the exportation of land data based on the specified export type.

-   `land_name`: A character string specifying the name of the land.
-   `export_type`: A character string specifying the type of export. Options include `"pagecsv"`, `"fullpagecsv"`, `"nodecsv"`, `"mediacsv"`, `"pagegexf"`, `"nodegexf"`, or `"corpus"`.
-   `minimum_relevance`: A numeric value specifying the minimum relevance score for inclusion in the export. Default is `1`.
-   `labase`: A character string specifying the name of the database file. Default is `"mwi.db"`.

**Example:**

This example demonstrates exporting data for the project "giletsjaunes" with a minimum relevance score of 3 into a GEXF file.

```{r}

export_land("giletsjaunes", "pagegexf", 3)
```

## Step 4: Enrich Your Corpus with SerpAPI Helpers

Once the foundational land is in place, the next objective is to broaden your web perimeter. The package provides dedicated helpers around [SerpAPI](https://serpapi.com/) so you can script keyword expansion and SERP harvesting before every crawl.

### 1. Discover Related Queries

```{r}
related_query("intelligence artificielle", lang = "fr", country = "France")
```

`related_query()` returns the "People also search for" block as a tidy data frame. Typical workflow:

-   collect the suggestions, inspect them quickly in R;
-   fold the most relevant ones back into `addterm()` so the next crawl covers allied narratives;
-   archive the CSV for methodological transparency.

### 2. Capture Google, DuckDuckGo, and Bing Result Lists

```{r}
urlist_Google(
  query = "ai OR artificial intelligence",
  datestart = "2024-01-01",
  dateend   = "2024-03-31",
  timestep  = "month",
  sleep_seconds = 2,
  lang = "en"
)
```

`urlist_Google()`, `urlist_Duck()`, and `urlist_Bing()` paginate SERP responses and write raw URL dumps on disk (one file per query). You can then read those files back with `importFile()` and feed them to `addurl()`. Remember to space requests (`sleep_seconds`) to stay inside rate limits.

### 3. Monitor SEO Signals

```{r}
mwir_seorank(
  filename = "aiwatch_seo",
  urls     = c("example.com", "opencorpus.org"),
  api_key  = Sys.getenv("SEORANK_API_KEY")
)
```

`mwir_seorank()` queries the SEO Rank API for MOZ/PageSpeed style indicators. Because the function appends rows as soon as a response arrives, you can launch it overnight on dozens of domains and obtain a ready-to-share CSV.

## Step 5: Clean and Structure Newly Found URLs

Raw SERP captures and manual curation often contain duplicates, broken links, and media files. The toolkit ships with a mini cleaning pipeline so your database only stores actionable pages.

### 1. Import URL Lists Interactively

```{r}
urls <- importFile()
head(urls)
```

`importFile()` opens a file picker (CSV/TXT) and normalises the result into a data frame. Use it when colleagues send URL batches by email.

### 2. Normalise Links and Detect Media Assets

```{r}
clean_url("https://site.org/article?id=42#section")
is_media_link("https://cdn.site.org/logo.png")
```

-   `clean_url()` trims anchors and exotic characters so duplicates collapse early.
-   `is_media_link()` tags images, documents, and audio/video files. Mix both inside `detect_links_and_add()` to store embedded media separately from HTML pages.

```{r}
detect_links_and_add(
  con        = connect_db(),
  content    = html_string,
  parent_id  = 123,
  land_id    = 5,
  urlmax     = 50
)
```

### 3. Rescue Content When the Primary URL Fails

```{r}
archive_info <- archive_available("https://example.com/article")
get_last_memento_url("https://example.com/article")
```

The pair `archive_available()` / `get_last_memento_url()` coordinates with the Internet Archive to fall back to preserved versions before giving up on a source. Combine them with `PDFtoText()` to extract machine-readable content from PDFs when the HTML is absent.

### 4. Produce Clean Metadata and Text Files

```{r}
slugify("IA Travail 2024 : Communiqué de presse")
clean_string("\tUne\nchaine\nde caractères\n")
```

`slugify()` standardises filenames, while `clean_string()` strips control characters prior to export. Together with `to_metadata()` they underpin `export_corpus()`, which yields plain-text dossiers ready for qualitative coding suites.

## Step 6: Build Dictionaries and Score Relevance

Text scoring relies on carefully curated lexicons. The following helpers accelerate dictionary work directly in R.

### 1. Tokenise and Stem

```{r}
phrases <- phrase_tokenizer("IA, travail, transformations du travail")
lemmas  <- mkdictionary(phrases, language = "fr")
```

-   `phrase_tokenizer()` splits comma-separated concepts;
-   `word_tokenizer()` and `stem_word()` operate at word level;
-   `mkdictionary()` aggregates the stemmed phrases into a tidy data frame you can store in your land dictionary table.

### 2. Detect Language Before Scoring

```{r}
mwiR_detectLang(
  df        = expressions,
  variables = c("title", "description"),
  engine    = "cld3",
  min_prob  = 0.75
)
```

`mwiR_detectLang()` guards against mixing languages when dictionaries are language-specific. Switch to the fastText backend whenever you have a custom model (`engine = "fasttext"`).

### 3. Compute Expression-Level Relevance

```{r}
scores <- expression_relevance(
  dictionary = lemmas$lemma,
  expression = expressions[1, ],
  language   = "fr"
)
```

`expression_relevance()` weights title and body occurrences, returning a single score that you can persist to the `Expression` table. For granularity, call `get_relevance()` directly on text fragments.

## Step 7: Transform and Diagnose Numeric Variables

When the time comes to model or discretise quantitative indicators (e.g., in-degree, frequency, sentiment scores), the package offers statistical helpers inspired by social-science workflows.

### 1. Explore Transformations Visually

```{r}
plotlog(
  df         = analytics,
  variables  = c("in_degree", "reach"),
  trans_type = c(in_degree = "log1p", reach = "zscore"),
  save       = TRUE
)
```

`plotlog()` overlays the original and transformed histograms so you can compare scales immediately. Main arguments and expected inputs:

-   `df` — data frame passed to the function. If you leave `variables = NULL`, every **numeric** column in `df` is analysed.
-   `variables` — character vector that specifies the columns to plot. You can supply a named vector or list so each variable receives its own transformation rule.
-   `trans_type` — transformation applied to each series. Recognised keywords: `"none"`, `"log"`, `"log1p"`, `"sqrt"`, `"rank"`, `"zscore"`. Provide a single value to reuse it everywhere, a named vector/list to mix them, or a custom function returning a numeric vector.
-   `bins` — histogram resolution. Accept an integer (e.g. `30`) or one of the standard rules: `"sturges"` (default), `"fd"`/`"freedman-diaconis"`, `"scott"`, `"sqrt"`, `"rice"`, `"doane"`, `"auto"` (maximum of Sturges and F-D).
-   `colors`, `alpha` — choose the two fill colours (original vs transformed) and set the transparency level between 0 and 1.
-   `theme` — any `ggplot2` theme object (`theme_minimal()` by default).
-   `density`, `show_rug` — booleans that toggle a kernel density overlay and a rug showing individual points.
-   `na_rm`, `min_non_missing` — control filtering. `na_rm = TRUE` drops non-finite values before plotting; `min_non_missing` (default 5) is the minimum number of finite values required for a variable to be plotted.
-   `shift_constant` — positive offset automatically added before log/sqrt transformations when the data contains zero or negative values (default 1).
-   `display` — `TRUE` prints the combined panel to the current graphics device; set to `FALSE` to return the object silently.
-   `save` — set to `TRUE` to export the plots. Use with `save_dir` (folder), `save_format` (`"png"`, `"pdf"`, or `"jpg"`), `save_dpi`, `device_width`, and `device_height` to control the files that are written.
-   `verbose` — produces progress messages when `TRUE` (defaults to `interactive()`).

### 2. Apply Robust Transformations Programmatically

```{r}
scaled <- transform_variable(
  x         = analytics$reach,
  method    = "yeojohnson",
  winsorize = 0.01
)
```

`transform_variable()` stores both the transformed values and the inverse mapping. This makes it easy to export model-ready columns while keeping de-standardisation metadata.

-   `x` — numeric vector to transform (NA/Inf allowed; non-finite entries are carried through).
-   `method` — choose the transformation: `"auto"` (bestNormalize search), `"none"`, `"center"`, `"zscore"`, `"robust_z"`, `"log"`, `"log1p"`, `"sqrt"`, `"boxcox"`, `"yeojohnson"`, `"ranknorm"`, or pass your own function.
-   `winsorize` — optional proportion (0 to <0.5) of tails to shrink before transforming. Set to `NULL` to disable (default).
-   `shift_constant` — positive value added before log/sqrt transforms when the series contains non-positive values (default 1).
-   `handle_na` — either `"keep"` (default; NA remain in the output) or `"omit"` (drop NA before estimating transformation parameters).
-   `...` — forwarded to `bestNormalize` when relevant (e.g. Box-Cox options).

### 3. Segment Indicators into Meaningful Classes

```{r}
clusters <- find_clusters(
  analytics$reach,
  max_G         = 5,
  transform     = "auto",
  winsorize     = 0.01,
  return_breaks = TRUE
)

classes <- discretize_variable(
  analytics$reach,
  method = "manual",
  breaks = clusters$breaks,
  labels = c("Faible", "Moyen", "Élevé")
)
```

-   `find_clusters()` ajuste des mélanges gaussiens 1D pour révéler des typologies. Principaux réglages : `max_G` (nombre de composantes testées),
    `criterion` (`"bic"` ou `"icl"`), `transform` (`"none"`, `"log1p"`, `"yeojohnson"`, `"zscore"`, `"auto"`) et `winsorize` (0–0.5).
    Avec `return_breaks = TRUE`, la fonction fournit des bornes prêtes à l’emploi. Vous récupérez aussi la `classification`, les
    probabilités `posterior`, le nombre de classes retenu et plusieurs diagnostics (taille des clusters, entropie…).
-   `discretize_variable()` convertit ensuite la mesure continue en classes parlantes. Selon `method`, vous obtenez des coupures
    `"equal_freq"`, `"equal_width"`, `"quantile"`, `"jenks"`, `"kmeans"`, `"gmm"`, ou vos propres seuils via `"manual"`.
    Le résultat est un facteur ordonné enrichi d’un attribut `discretize_meta` (bornes, effectifs, avertissements) pour alimenter vos rapports.

### 4. Examine Heavy-Tailed Behaviours

```{r}
powerlaw <- analyse_powerlaw(
  analytics$reach,
  type             = "discrete",
  candidate_models = c("powerlaw", "lognormal", "exponential"),
  bootstrap_sims   = 200,
  winsorize        = 0.01,
  threads          = 4
)
```

-   `analyse_powerlaw()` confronte plusieurs lois de queue pour tester la présence d’une véritable loi de puissance.
-   En mode `"discrete"`, les valeurs sont automatiquement arrondies et les zéros supprimés ; assurez-vous d’avoir suffisamment d’observations positives (`min_n` par défaut = 50).
-   Ajustez `type` (`"discrete"` ou `"continuous"`), la combinaison `candidate_models`, les paramètres de robustesse (`winsorize`, `xmin`) et `threads` (nombre de cœurs alloués au bootstrap) selon vos contraintes de calcul.
-   `candidate_models` accepte, en fonction du type : `"powerlaw"`, `"lognormal"`, `"exponential"` côté discret, et ajoute `"weibull"` pour le continu. Fournissez un sous-ensemble pour tester uniquement les lois pertinentes à votre terrain.
-   `bootstrap_sims` pilote le nombre de simulations KS ; `bootstrap_models` permet de restreindre les modèles testés. Réduisez `bootstrap_sims` pour obtenir un diagnostic rapide, augmentez-le pour un verdict plus solide.
-   La liste retournée synthétise le modèle retenu (`best_model`), les paramètres (`best_fit`), les comparaisons de vraisemblance (`comparisons`), les diagnostics bootstrap (`bootstrap`) et un résumé (`data_summary`) directement mobilisable dans vos rapports.
-   Bonnes pratiques : tester plusieurs valeurs de `winsorize`, surveiller `best_fit$n_tail` (taille de l’échantillon utilisé au-dessus de `xmin`) et examiner les p-values bootstrap pour juger de la qualité de l’ajustement.

## Step 8: Leverage AI Assistance Responsibly

Large Language Models can speed up qualitative coding, but they demand guardrails.

```{r}
Sys.setenv(OPENAI_API_KEY = "sk-...")
gpt_out <- GPT_Recode(
  prompt      = "Traduire en français",
  cell        = "Automation and labour markets",
  model       = "gpt-4o",
  temperature = 0.4,
  max_tokens  = 120
)

Sys.setenv(OPENROUTER_API_KEY = "orpk-...")
or_out <- OpenRouter_Recode(
  prompt          = "Résumer en 20 mots",
  cell            = "The platform reorganised work across the supply chain",
  model           = "openrouter/auto/gpt-4",
  temperature     = 0.2,
  max_tokens      = 80,
  return_metadata = TRUE
)
```

**Entrées de `GPT_Recode()`**  
- `prompt` : consigne textuelle (obligatoire).  
- `cell` : contenu à transformer (caractère, longueur 1).  
- `sysprompt` : message système pour cadrer le modèle (défaut : demander une réponse courte).  
- `model` : identifiant OpenAI (ex. `"gpt-4o"`, `"gpt-4o-mini"`).  
- `temperature` (0–2) : aléa de génération.  
- `max_tokens` : limite de tokens en sortie.  
- `max_retries`, `retry_delay` : politique de relance en cas d’échec réseau ou quota.  
- `validate` : lorsqu’il est `TRUE`, la sortie est filtrée (longueur raisonnable, absence de messages d’erreur).  
> **Prerequis** : la variable d’environnement `OPENAI_API_KEY` doit être définie avant l’appel.

**Sortie de `GPT_Recode()`**  
- Une chaîne de caractères avec la valeur recodée.  
- `NA` est renvoyé si l’API échoue après les relances ou si la validation estime la réponse incohérente (dans ce cas un `warning` est émis).

**Entrées de `OpenRouter_Recode()`**  
- `prompt`, `cell`, `sysprompt`, `model`, `temperature`, `max_tokens`, `max_retries`, `retry_delay`, `validate` (mêmes rôles que ci-dessus).  
- `referer`, `title` : en-têtes requis par OpenRouter pour l’analytics.  
- `api_base` : URL de l’endpoint (défaut : `"https://openrouter.ai/api/v1/chat/completions"`).  
- `timeout` : durée maximale de la requête (secondes).  
- `extra_headers` : vecteur nommé pour ajouter des en-têtes HTTP personnalisés.  
- `return_metadata` : quand `TRUE`, la fonction renvoie aussi les métadonnées HTTP (statut, en-têtes) pour archivage.  
> **Prerequis** : définir `OPENROUTER_API_KEY` et disposer des packages `httr` et `jsonlite`.

**Sorties de `OpenRouter_Recode()`**  
- Par défaut, une chaîne de caractères.  
- Avec `return_metadata = TRUE`, une liste contenant :
  - `value` : la réponse du modèle ou `NA` en cas d’échec,  
  - `status` : informations sur la validation et le nombre de tentatives,  
  - `http` : code de statut et en-têtes de la réponse HTTP.

**Bonnes pratiques**  
- consigner `prompt`, `sysprompt`, modèle et version dans votre journal de recherche ;  
- fixer `temperature` bas (≤ 0.3) pour des traductions fidèles, et l’augmenter pour des reformulations plus créatives ;  
- limiter `max_tokens` pour éviter les réponses prolixes ;  
- surveiller les quotas et gérer les `NA` dans vos scripts (utiliser `if (is.na(result)) …`).

## Step 9: Maintain the Database Throughout the Project Lifecycle

The database layer underpins every land. The following helpers keep it healthy and synchronised with external edits.

### 1. Connect Programmatically and Reuse IDs

```{r}
con      <- connect_db()
land_id  <- get_land_id(con, "AIWork")
domaines <- list_domain(con, land_name = "AIWork")
```

-   `connect_db()` returns a ready-to-use `DBI` connection.
-   `get_land_id()` converts human-readable land names into numeric IDs when you automate workflows.
-   `list_domain()` produces a domain summary (counts, keywords) to monitor coverage.

### 2. Import Additional Material

```{r}
urls <- importFile()
addurl("AIWork", urls = urls$url)
```

Use `importFile()` whenever you enrich your corpus from spreadsheets or open postings. The helper returns a data frame; pass the relevant column to `addurl()`.

### 3. Reinstate Externally Annotated Data

```{r}
annotatedData(
  dataplus = curated_notes,
  table    = "Expression",
  champ    = "description",
  by       = "id"
)
```

`annotatedData()` wraps transactional updates so a batch edit either fully succeeds or rolls back. Always back up `mwi.db` before bulk reinsertion.

### 4. Export Precisely What You Need

Beyond `export_land()`, the family of dedicated exporters gives you fine-grained control:

-   `export_pagecsv()` and `export_fullpagecsv()` to share tabular corpora;
-   `export_nodecsv()` / `export_nodegexf()` for network analysis;
-   `export_mediacsv()` to audit associated media;
-   `export_pagegexf()` for expression-level graphs;
-   `export_corpus()` to assemble text files plus metadata headers (ideal for CAQDAS tools).

Each exporter accepts `minimum_relevance`, so you can balance breadth and focus depending on the audience.
